["big_input.txt", ""]
["output1.txt", "hi I'm Dan jurafsky & Chris Manning and I are very happy to welcome you to our course on natural language processing this is a particularly exciting time to be working on natural language processing the vast amount of data on the web and social media have made it possible to build fantastic new applications let's look at one of them question-answering you may know that IBM's Watson won the Jeopardy Challenge on February 16th 2011\r\n I'm answering questions like William Wilkinson's book inspired this authors most famous novel and you may know that the answer is Bram Stoker who famously wrote\r\n Dracula\r\n another important task is information extraction for example of imagine that I have the following email from iCarly Chris about scheduling a meeting\r\nwe'd like software to automatically notice that there are dates like tomorrow times like 10 to 11:30 and a room like Gates 159 extract those information create a new calendar entry and then populate a calendar with this kind of structured information with the event date start and end for calendar program in modern email and calendar programs are capable of doing this from text\r\n another application of this kind of information extraction involve sentiment analysis imagine that you're interested in cameras in your reading a lot of reviews of cameras on the web for here's a bunch of bunch of reviews we'd like to automatically determine from the reviews that would people care about him cameras are particular attributes if they're buying a camera they want to know if\r\nPinterest so we can see the main purpose of raising don't see but that parse receipt right away is that it's not raises that's the main verb of the sentence but interest somebody interest something and then that's something that gets interested is rates\r\n what is interesting these rates well\r\n it's fed raises raises by the Fed so the complete different setting for the different interpretation that something is interesting the rates whatever that could mean and it seems unlikely interpretation for people but of course for a parser this is a perfectly reasonable interpretation that we have to learn how to rule out in fact the sentence can get even more difficult this is the actual headline was somewhat longer so we had\r\nEd raises interest rates half a percent here we could imagine that rates is the verb and now we have what is rating fed raises interest interest in federal raises R rating half a percent so we might have a dependency structure like this so again interest rates the raises are what do the interesting in the FED is a modifier raises so whether with her face trucks or cars or dependency pars and even more so as we add more words when get more and more ambiguity that have to be solved in order to build a parts for each sentence\r\nthe format of the course you're going to have fun video quizzes and most lectures will include a little quiz and they're there just to check basic understanding there simple multiple choice questions you can retake them if you got them wrong\r\nT1 right now number of other things make natural language understanding difficult\r\n one of them is the non-standard English that we frequently see in text like Twitter feeds or we have capitalization and unusual spelling of words and hashtags and user IDs and so on so all of our partners in part of speech tigers that were going to make you supper often trained on very clean a newspaper text English but the actual English in there in the wild or I will call this a lot of problems I have a lot of segmentation problems for example if we see the the string York news part of New York New Haven how do we know the correct segmentation is New York and New Haven so the New York New Haven railroad and not something like\r\n York - New\r\nthis word here is not a word like in dash long after I saw the segmentation problem correctly you have problems with idioms and with new words that haven't been seen before and will also have problems with entity names like the movie A Bug's Life which has English words in it until it's often difficult to know where the movie names starts and ends and it's comes up very often in biology we have genes and proteins named with English words understanding it's very difficult what tools do we need but we need knowledge about language knowledge about the world and a way to combine these knowledge sources so generally the way we do this is to use probabilistic models that are built from language data so for example if we see the word Maison in French for very likely to translate that has the word house in English the other hand if we see the word\r\nEisenhower in French very unlikely to translate that as the general avocado and training these probably stick models in general can be very hard but it turns out that we can do an approximate job of probabilistic models with rough text features and will introduce those rough to text features as we go\r\n so I go on the class is teaching KI theory and methods for statistical natural language processing we'll talk about the viterbi algorithm naive Bayes and Max and classifiers who introduced and gram language modeling statistical parsing we'll talk about the inverted index and tf-idf in Vector models of meaning that are important information retrieval\r\n will do this for practical robust real world applications will talk about information extraction about spelling correction about information retrieval\r\nthe skills you need for the task you'll need simple linear algebra so you should know what a vector is and what a matrix is should have some basic probability Theory need to know how to program an either Java or python because they'll be weekly programming assignments you have your choice of languages\r\n we're very happy to welcome you to our course on natural language processing and we look forward to seeing you in following lectures\r\nZoomer affordability your size and weight Buttes and then we'd like to automatically for any particular action to determine how the reviewers felt about those attributes for example if a reviewer said nice and compact to carry that's a positive sentiment and there's another positive example but a phrase like flimsy is a negative sentiment which sentence with the sentiment is and then aggregate for each features for a safe resume for Ford abilities with might decide that this camera reviews really like the flash but they weren't so happy about the use of you so we might measure the positive and negative sentiment about each attribute and then aggregate those machine translation is another important new application and translation can be fully automatics\r\nexample we might have a source sentence in Chinese and he was at Stanford's phrasal and t-system translating guide into English but empty can also be used to help human translators so here we might have an Arabic text and the human translator translated into English might need some help from the Mt system for example of a collection of possible next words that the empty system can build automatically and help the human translator let's look at the state of the art in language technology like every field and he's divided up into Specialties and subspecialties number of these problems are pretty close to solve so for example spam detection well it's very hard to completely to text spam you her email boxes we don't have 99% spam and that's because spam detection is a relatively easy classification task\r\na couple of important component tasks part of speech tagging a named entity tagging will talk about those later in the course and those work at pretty high accuracy using to get 97% accuracy and part of speech tagging and we see how that's important for parts and we're making good progress not as commercial not as completely solved but there are systems out there that are that are being used so we talked about sentiment analysis the task of deciding thumbs-up or thumbs-down on the sentence or product\r\n component Technologies like Word Sense disambiguation deciding if we're talking about a rodent or a computer mouse when people talk about mouses in a search then we'll talk about parsing which is good enough now to be using lots of applications and machine translation usable on the web applications however are still quite hard\r\nfor example answering hard questions like how effective is this medicine in treating. Disease by looking at the Weber by summarizing information we know it was quite hard similarly when we made some progress on the siding that the sentence XYZ company acquired ABC company yesterday mean something similar to ABC has been taken over by XYZ the general problem of detecting that two phrases or sentences mean the same thing to paraphrase to ask still quite hard even harder is the task of summarization reading a number of what's a news article that say oh the Dow Jones up or the S&P 500 is jumped and housing prices Rose and aggravating that to give a user information like in summary the economy is good\r\n and finally one of the hardest tasks in natural language processing\r\ncarrying on a complete human machine communication and dialogue so here's a simple example asking about what movie is playing when and buy movie tickets and you can get applications that do that today the general problem of understanding everything the user might ask for and returning a sensible response is quite difficult\r\n why is natural language processing so difficult are the kinds of ambiguity problems that are called crash blossoms Lamborghinis any case were surface for might have multiple interpretations a crash Blossom is the name for a kind of headline that has two meanings and ambiguity causes humorous interpretation So reading this first headline violinist link to JL crash blossoms\r\nyou might think that the main verb is linked and a violinist is being linked to what he's been linked to Japan Airlines crash blossoms what what are crash blossoms well this headline gave the name to this phenomenon Because the actual interpretation that the headline writer intended the main verb was blossoms who does the blossoming a violinist and this fact about being linked to JL crash with a modifier violinist\r\n similar kinds of syntactic ambiguity so here teacher strikes Idol kids the writer intended the main verb to be idle the strikes cause the kids to be idle but of course the humorous interpretation is that the teacher is striking strike is the verb and we have a teacher\r\n striking Idol kids\r\nambiguity is word since ambiguity\r\n so when our third example red tape holds up new Bridges the writer intended holds up to mean something like delay call that since one of them holds up but the amusing interpretation is the second sense of holds up which we might write down his to support\r\n and now we get the interpretation that literal red tape it supposed to be or Craddock red tape is actually supporting a bridge that we can see lots of other kinds of ambiguities in these actual headlines now it turns out that it's not just amusing headlines that have ambiguity ambiguity is pervasive throughout natural language text what's look at a sensible non ambiguous looking headline from The New York Times to the headline with shortened it here it is\r\nand raises interest rates we have a verb here already low parse tree raises what gets raised\r\n a noun phrase or write a little too nouns here interest rates and we'll have a verb phrase so raising interest rates and then we'll have the Fed\r\n Megalo noun phrase and I will say this is a sentence that has a noun phrase and a verb phrase raises interest rates so this is called a phrase structure pars we'll talk about that later in the course phrase structure\r\n so we can also write a dependency far so we see the head verb raises has an argument which is fed and has another dependent which is rates and rates have another itself has a depend\r\n"]
["output10.txt", "how are we going to compute and I'm at a distance\r\n the standard algorithm is with dynamic programming dynamic programming is a tabular method of computation what we're going to do is we're going to compute D the distance between two strings X & Y axis length in wavelength M by combining solutions to sub problems and come by and solutions to sub problem is the intuition of all dynamic programming algorithms is very simple where can I buy small prefixes of length I of string X & J of string why will compute the distance between those strings and what computer are larger distances for larger strings based on those previously computed similar values other words we're going to compute the distance i j between prefixes of string acts of length\r\nand prefixes of string y of length J for all I and J will end up in the end with the DM distance\r\n so let's look at the actual equation here's the equation for defining minimum at a distance and I've given you Levenstein distance which is the distance when there is a cost of 1/4 insertions 1/4 deletions and 14724 substitutions excuse me\r\n so let's look at the initialization condition so the day we first say that any characters in X so this is the X string the eye characters in X string the distance between those and the null string and why is the cost of deleting each of those characters so the cost\r\nthose his is the length of the string were deleting each character and similarly for inserting all the characters in 2y to create the string why the distance between the null string of X and the string twice just the length of why the insertion cost of why the recurrence relation so walking through string Axe and walking through string why\r\n will have the distance in any particular cell of our Matrix is going to be the minimum way of getting to that sell from three previous cells if we go from the string I that's one short person would deleting\r\n one more thing and I\r\n to make a j or we're inserting one thing into J to make it longer or were substituting between the previous string\r\nI have length I'm exiting 5 - 1 + y of length J - 1 we're adding a new character if it's the same in both strings we have a cost of 0 if it's different we have a substitute cost of 2 and then at the end the distance between the two strings is simply the is simply the the D of NM the upper right corner of the Matrix so here's our table\r\n and we can fill in each element of the table from using this equation that tells us the deletion cost the insertion cost and the substitution cost\r\n so what to do that I put the equation up here up here in the corner so we want to know what's the distance between the nail string of intention of the\r\nflexicution obviously zero the null string the string I to the string nothing is still cost of deleting and I that's one\r\n so now it's for the compute what's the cost of converting in to eat while intuitively we expect it's going to be a deletion in a substitution so let's see if that works out right so feed them\r\n element in the cell is the minimum of three values it's this distance plus one this distance plus one or this distance Plus either too if I N 0 or different or 0 if they're the same well they're different so it's the minimum of I + 1 which is 201 + 1 which is 2 or 0 + 2 which is\r\nso we have to so we can write to in this cell\r\n similarly if we want to know the distance between I N & E it's the minimum distance of I am to nothing + 122 + 1 or 3\r\n or the different distance between i and e plus the cost of adding in that end or 3\r\n or the cost of having just an eye and adding in that and Touhy substitution which is two or three so again we have three years we have a 2 and we have a 3 and if we continue along this manner again in each case with me the three previous sales and using this equation over here will slowly end up with\r\nsalon\r\n and if we continue along this in this manner we're going to end up with the following and complete table so every selling this table let's take this cell tells you the cost of sub of the day at a distance of editing the string int e and turned into the string exe and that means that this value here in the upper right corner is the cost the edit distance between intention and execution the cost of turning intention and execution and we see the value in which week earlier said was the Levenstein distance so we have Levenstein distance equals 8\r\nthat's our algorithm for computing minimum at a distance\r\n"]
["output11.txt", "knowing the edit distance between two strings is important but it turns out not to be sufficient we often need something more which is the alignment between two strings we want to know which symbol in string X corresponds to which symbol and string wine this is going to be important for any application we have a better distance from often from spell-checking to machine translation even in computational biology is we keep a back trace the Backstreets is simply a pointer when we enter each cell in The Matrix that tells us where we came from and and when we reach the and the upper right corner from Matrix we can use that point then Trace back through all the pointers to read off the alignment\r\nlet's see how this works in practice again I've given you the equation for each cell in edit distance\r\nand if we put in some of our values that we saw earlier I'll start by putting in some values so\r\n all right so we can ask how did we get to this value 2\r\n two is that we picked a minimum of three values we could either take so to is the distance this to hear the distance between the string I in the stringy and we got that by saying it's either the alignment between nothing and e-plus the insertion of an extra eye\r\n so that's dumb distance of 1 + 1 is 2 or 0 + 2 is 2 or one plus one is two so we had three different\r\nuse so we were asking which of which minimum path did we come from really they're all the same we could have come from any of them and that's going to be true for this value 3 as well it we computed as the minimum of 2 + 101 + 2 or 2 + 1/2 this could come from here here or hear and similarly that's going to be true I didn't work out the Rhythm take for you but it's going to be true for the cell to you can work it out for yourself here we have a diff distance difference so the distance between I N T E & E we could compute that by taking the distance\r\nbut it cost us to to them convert int eaten nothing and then add another insertion for eBay that would be that would be silly because 4 + 1 is 5 and there's a cheaper way\r\nto get from int to eat and that is that it cost us nothing to match this E2 that eat so our previous alignment between int and nothing we we can have zero from 3 to get at 3 so\r\n the minimum path for this 3 came from that 3 so well in some cases sell came from any places in this case it on him to goosley came from this previous three until we're going to do this for every cell in the array and the results will look something like this where we have for every cell in every place it could have come from and you'll see that in a lot of cases any path could have worked so the 6 could have come from any place but crucially the spinal alignment is 8 that tells us the final edit distance between intentional\r\nare Trace back tells us it came from the best alignment between intentio and execution which came from the best alignment from intense I from execute I and so on I'm in so we can trace back this alignment and get ourselves alignment that tells us that this an match this and and this match this so and so on but maybe here we have an insertion rather than the clean lining up\r\n Computing the back Trace very simple\r\n we take our same minimum edited since I wasn't that we've seen and here I've label the cases for you so when we're looking to sell we're either deleting inserting or substituting and we simply add pointers so in the case where where inserting leap\r\nleft in the case where were deleting Point down in the case where I'm substituting Point diagonal you have shown you that arrows on the previous slide\r\n so we can look at this distance Matrix and think about the pads from the origin\r\n here\r\n to the end of The Matrix and any non decreasing path that goes from the origin to the point then NM corresponds to some alignment of the two sequences\r\n an optimal alignment then\r\n is composed of optimal subsequences and that's the idea that makes it possible to use dynamic programming for this task\r\nso the resulting of our back-trace are two strings and then the alignment between them so we will will know which wich things lined up exactly which things line up with substitutions and then when we should have insertions or deletions\r\n what's the performance of this algorithm\r\n in time it's order NM because our distance Matrix is of size n m\r\n we were telling each cell one time. The same is true for space and then the back Trace then we have to in the worst-case go for if we had an deletions and M insertions we have to go then and plus em with the touch and + m cells but not more than\r\nthat's our back Trace algorithm for computing alignments\r\n"]
["output2.txt", "at a distance can also be weighted why would we add weights to the computation of edit distance\r\n I think about taking applications in spell correction it's obvious that some letters are more likely to be Miss type than others while in biology because of the constraints of the subject matter some kinds of deletions insertions are more likely than others so for example and spelling there's a confusion Matrix for spelling errors so if you look at this confusion Matrix you can see that he is very likely to be confused with a or o a nice ovalles tend to be confused\r\n but it's very unlikely to come to confuse A and B so as are confused with ease and eyes and O's and use\r\nthey're the kind of spelling errors people make have system atticity to them so not just confusing vowels with vowels\r\n but also the fact of the keyboard means that you're likely to make errors either using the homologous finger on the other hand or using nearby keystrokes so the constraints of the domain in this case we're talking about the spelling of maybe I can buy a biology I got to make some added to some edits more likely than others\r\n so we got to represent this by modifying the algorithm slightly to add weights so in levenshtein-distance we have the cost of one for insertion one for deletion and two for substitution and waited Minima to since we simply add a special cost of you look up each time so initialization instead of just adding one for each deletion we have the actual cost of each day\r\nand we add up the deletions of each of the symbols that we delete instead of having just one for insertion we have a cost for each insertion we add them all up and similarly in the recurrence relation we're going to add a special deletion insertion and substitution cost how much does it leak to delete that particular character insert that particular character and then we'll end up with the same termination condition so we'll just add separate look up tables that will tell us what the deletion insertion and substitution cost are for each symbol\r\n by the way where did the name dynamic-programming come from\r\n here's some quotes from Richard bellman's autobiography Bellman was the one who invented dynamic programming and amusingly he tells us that he came up with the name for them and really as a public relations move to make an algorithm sound excited\r\nso this is the only be one of the first algorithms that was that was named them in a branding way to make an algorithm sound exciting\r\n so there is our in summer algorithm for weighted minimum at a distance\r\n"]
["output3.txt", "their number of advanced Burien submit my medicines that play a special role in computational biology Sol recalled in computational biology what were aligning is sequences of nucleotides or sometimes proteins in our job is to take them two strings like this and produce an alignment like this\r\n so in biology this is important for a number of\r\n reasons we can be I'm finding regions in the in the genome we could be discovering functions of genes we could be looking for evolutionary things by comparing different species this is also important for assembling fragments of DNA sequencing we're going to be trying to assemble fragments we want to look for overlapping pieces will talk.\r\nblack pieces and find some matches between them and find that these two pieces match I'm and we're comparing individuals and looking for mutations finding places where there are similarities and differences\r\n I mean General in natural language processing we talk about distance string at a distance and then I'm headed distance to or minimizing distance and wear them Computing them weights for things in computational biology we are talking about similarities or maximizing similarities or ask me how similar to things are so we're trying to maximize something and we only talk about the scores rather than weights\r\n so interpretation biology the standard minimum at a distance Ogden that we've just looked at them is called needleman wunsch and I've shown you the\r\nlook for all the local alignments between these two strings a t c a t and a t t a m a t c\r\n Stampy fill in The Matrix I can start with zeros everywhere because we're doing local alignment.\r\n We see them too\r\n if we lend look for Regions cells that have a maximum distance to trace back from we see two of these cells\r\n so one of them\r\n corresponds to the alignment ATC 8280 that so we have four strings that match one mismatch so that's going to be distance of 3\r\n and the other one of the\r\nover here corresponds to the alignment between ATC and ATC where we have three matching at a symbols\r\n those are some of the more advanced variance of a distance that we see in computational biology\r\nthe album here but it's the same the same thing that we saw before although in general we're just going to keep them will use D2 mean the cost of insertions and deletions and we'll have a little value for the substitution the positive or negative value of substituting things in general in biology will talk about a positive cost for things that match a positive value for things that match and a cost for things up for deletions and insertions\r\n but here's the needleman wunsch Matrix and noticed that as opposed to what we did in natural language processing in general and computational biology we put the origin at the upper left\r\n so what's let's first\r\nit some very instead of important in computational biology so one is cases where it's possible to have unlimited gaps at the beginning and end of a string and this happens exactly. We have two Little Snips of DNA and we know that the endpoints of one might overlap with the ends of another but there might be something else going on in other places so I'm Yours one long sequins and here's another long sequence but it's just this piece of this sequence in this piece of this that might overlap so we don't want to penalize the fact that there's other things going on before here or after here so we like to modify the album so doesn't penalize gaps at the end\r\nin fact I can be various different kinds of overlapping of this of this sort this might happen when we were when we were doing sequencing and we have overlapping weeds\r\nor it might be that we're looking for a piece of a gene inside another piece and so we have a subset piece inside a larger piece\r\n so the variant of the dynamic programming algorithm that we use for overlap detection the overlap detection variant will just make a few small changes in the algorithm so first and we just change the initialization so that m\r\n it doesn't cost us anything to start from a long string and delete everything over insert everything so used to be that we had we had minus minus I stardy here and we had - J star.\r\nhere and we gotten rid of those because it's allowing ourselves to start at path at a random Point way out here in the intersection so we're we're allowing ourselves to start at zero cost here and not be penalized for not matching all these things up till here so we looking for again Edge overlaps\r\n and now then for our termination condition we're going to look for the them start from not from the upper right corner because we're allowing a match not to go all the way to the edge but go find the the place along the final column or the final row where we have the maximum value and will Trace back from there so in this case or maximum value is he\r\nopen this last column and will Trace back from there\r\n similar extension of the Emile minute lunch or the standard dynamic programming algorithm for string at a distance is the local alignment problem so here's the local alignment problem we have two strings\r\n acts of links m&y of links and we want to find to substrings who's similarity is maximum so imagine that hears X and here's why we'd like to add of this these two strings we'd like to find these two and B substance cccggg that's the largest I'm similar substring\r\n so it's it's very similar to\r\nBM overload protection Grant we saw except not only do we allow ourselves not to to ignore previously on aligned sequences at the beginning and end but also anywhere so we can we can basically have our maximum alignment be somewhere in the middle as it is here\r\n so in order to in order to modify the needleman wunsch algorithm them to allow any kind of luck alignments this is the new version is now called the Smith Waterman algorithm and we're first going to allow as we did for the time overlap detection Burien that allow them the initialization conditions to be zero both for X and Y so we don't p.m.\r\nourselves for initial strings and now we're going to make one more modification which is that in each cell when we're looking at the possible and places we could come from to choose the alignment we're going to not only pick the maximum of the three previous cells are also going to add a maximum of 0 so we're going to let ourselves since in in biology were talking about maximizing similarity when things get very different ones but very negative score we just going to start all over again from zero allow myself to just throw away region align it all\r\nthe termination condition of the Smith Waterman algorithm depends on what we're looking for if we just want the best local alignment\r\ncheck the place that's that's maximum in the entire array and will Trace back from there\r\n if we want all the local alignments that score greater than some threshold T then maybe we'll find someplace that's greater than T. And find all those places and Trace back all of them now this gets complicated by the fact that there can be overlapping local alignment so yours might have to alignments like this and that and it might be that they actually overlap tracing back so they can be some complications here if you want the best local lineman that texting much easier\r\n so here's an example of local alignment so let's let's imagine that we're we're we're getting one positive point every time two symbols match and a negative point for any deletion insertion and substitution and\r\n"]
["output4.txt", "now we're going to talk about text processing the most basic and fundamental to we have for text processing is the regular expression expression is a formal language were looking for woodchucks in a text document and woodchucks can be expressed in a number of ways we can have a singular woodchuck we could have the plural S at the end we could have a capital letter at the beginning of a lowercase in any combination of these tools to deal with this problem\r\n so the simplest fundamental to the regular expression is the distinction the square brackets in a regular expression pattern mean any letter inside the square brackets so lowercase w capital W square bracket means either a lowercase w or capital W so we can combine that with woodchuck to match\r\nlowercase or uppercase woodchuck and similarly with digits 1 2 3 4 and so on 5 6 7 8 9 0 matches any digit that was kind of annoying to write so we'd like to do instead is have little ranges the range 0 3-9 to square bracket 0-9 means any character inside that range and the range a dash Z10 character between a capital letter between a and z let's see if we can see how that works so here's an example of a little too were going to use for regular Expressions searching and we have a little texture from dr. Seuss we looked then we saw him stepping on the mat we looked and we saw him the Cat in the Hat\r\nand let's try our our disjunctions so we can have\r\nfinding this this. Here has plans this thing or not. But it misses these two does Elsa find some other things let's fix the first problem and how do we not get the dogs in the middle of but those capitalize does at the beginning of what we're going to use our and\r\n are disjunction\r\n and sure enough that correctly now matches the to beginning of line does but you know star pattern although it now capture something at Miss before it still captures things that shouldn't be capturing other there and belies so we need to augment are patterns I hope there's not an alphabetic character around the meaning of space\r\npunctuation or something on alphabetic so let's just say non alphabetic\r\n afterwards great that gets rid of the other and their doesn't sell Blythe cuz the light has an alphabetic character before it lets go fix blive by saying non alphabetic before it's either\r\n there we go now we found all of our all of our does\r\n so we we looked for the we noticed it missed capitalized example so we we added some we made our pattern more more more expensive we increase the yield of our pattern but that incorrectly returns more things so then we'd eat we did mcspadden more precise by by specifying more things this process\r\nthat we went through is based on fixing two kinds of Errors when is matching strings we shouldn't have Matt's we match today are we matched other so that's trying to add that solving the problem of false positives are there called type 1 errors we were matching things we shouldn't match and the other thing we went through is to solve the problem of not matching things we should have nap so we miss those Capital does and that's dealing with the problem of false negatives are type 2 errors and it turns out in that in natural language processing we're constantly dealing with these two classes of Errors so reducing the error rate in any application were going to see this again and again in this course involves two antagonistic efforts we're increasing the accuracy or Precision which helps us minimize those false positives or minimizing or false negative\r\n7 summary regular Expressions play a surprisingly large role in text processing and the sophisticated sequences of regular Expressions that we've seen very simple versions of are often the first model for almost any text processing tasks for harder tasks we are offering going to be using and roll introduced these these machine learning classifiers that are much more powerful but it turns out even then regular expressions are used as features in the class of fires are very useful at capturing generalizations\r\nso you're going to be returning again and again to regular expressions\r\nthe capital W and lowercase w and she's me capital w in a lowercase w and that's going to match as you can see the capitol W's and the lowercase w is just fine\r\n or we can have all the Eevee's and all the emblems that's going to match all the ease in the atoms or in our ranges we can have all the capital letters\r\n is all the capital letters being matched we can have all the lower case letters\r\n it's lowercase letters there or we can match all of the alphanumeric characters think for a second how to match all of the alphanumeric characters can have\r\n or we can simply match some of the non alphanumeric characters we could have space and!\r\ndescribe a person that's going to match as you can see some of the no no cat characters okay so let's go on another kind of thing we might want to do in our regular Expressions is negation in our discussions we might want to say we don't want some kind of set of letters so for example we might want to say not a capital letter we can do that by saying carrot a through z in are square brackets carrot when it occurs right after the square brackets memes and not carry A to Z not a capital letter carrot a little a means neither a capital nor little a\r\nand carrot carrot means not an e and not a carrot so you can see what the carrot when it occurs right after the square bracket means not but later\r\nI mean simply just a carrot so let's take a look at that\r\n so we can try finding all of the non capital letters\r\n is all in on capital letters have it all done on exclamation points\r\n those things and the non alphanumerics\r\n sorry I didn't not alphabetics\r\n there's just two spaces in exclamation points so you can see\r\n how about looking for a carrot many carats in here there are none so there are no carrots into nothing yet nothing matches\r\n another kind of disjunction which can be used for longer strings is the pipe symbol sometimes\r\nor or pipe or just distinction so groundhog or woodchuck can beat will mean either the string groundhog or the string would woodchuck so we can use the pipe symbol sometimes for the same thing as the square brackets o a pipe B pipe C is the same as square bracket ABC\r\n and we can combine these things we can combine the square brackets in the pipe so we can have groundhog or woodchuck but use are square brackets for expressing capitalization at the beginning\r\n we can see that in our in our little examples we can have looked or step and sure enough they are the words look and stem are both highlighted or we can have sex like just random things you have to be words we can have all of the app\r\ngive me all of the apps and all the books and any Brandon string is fine sets of special characters that are very important regular Expressions them? Means that the previous character was optional so the? After this you hear me will match the word color with or without the u with without the u with the U\r\n enter the to clean the operators named for Steven cleaning cleaning star matches zero or more of the previous character so here's the star it matches 0 morrows so we have one followed by 0 or more other owes so there's the initial o and 0 other Rose and then RH! Here's our initial of followed by 100 and then a chance\r\ntwo three and someone sometimes more simple we can have them the cleaning plus so that means one or more of the previous character so there's Ro followed by the plus meaning one or more o\r\n so he's one of theirs two of those three hours and so on and the. Is a special character meaning in a character so begin can match begin begun bg380 matches anything\r\n and finally two special characters\r\n the current matches the beginning of the line so carrot capital A through Z matches a capital letter the beginning of the line the dollar sign matches the end of a line so A through Z dollar matches the end of a line like the capital letter at the end of the line and then\r\ndo you want to talk about it. It's in. There a special character we have to escape them then backslash. Means a. So. By itself means any character backslash. Means a real. Let's go look at some of these\r\n so here's the letter O here's 0 or if I can make it let's make it 1 tomorrow first here's one or more o\r\n so there's one over here and two o's over here\r\n then and now it's looking in begins and ends of lines here is\r\n capital letters at the beginning of the line\r\n here's\r\ncapital letters at the end of the line\r\n there aren't any\r\n here's punctuation at the end of the line\r\n there's only exclamation points at the end of the line\r\n here's all the periods normally up the backsplash are. And if we didn't backslash The. We would get all the characters cuz. Matches everything\r\n let's do one more example let's look at this little sentence hear the other one they are the Blythe one lets us walkthrough how to search for word let's find the word the other words I in this little passage something for yourself how you would do this for the simplest thing you might do is just type the the\r\n and that does a good job of\r\n"]
["output5.txt", "in research and Opie these days that's always a lot of talk of probabilistic models in machine learning but if you actually look at large systems under the hood what you almost always find it so they also make quite a bit of using regular expressions in various places and Ferb Mini Toss it turns out the regular Expressions. Just a very practical and capable way of specifying various kinds of natural language patterns I'm going to show you one example of this now I show you how we use regular expressions for the English tokenizer inside the Stanford NLP tools such as deposit part of speech tagging on for the coronal piece weed over roll\r\nokay hit me up with the code for the Stanford English took nyzzy so what it is is\r\nSolage deterministic regular expression and so what is my name is with the tool called Jay Flex so long to a family of what is commonly called in computer science Alexa is which is just another word to tokenize as which would take a sequence of characters in Cut pieces One Toke move time off the front of it so there was the original Alexa spot of Unix and then flex and then this is Jay Flex which is a job a compatible version let's go down to wear some of the regular Expressions I used to define character classes\r\nopen what you find is that many of the regular Expressions on actually very complicated that they really nothing more than list of input into regular Expressions by putting those\r\npause in between 4th nation and soap refillable we see that in several places here so here we have one for abbreviated months and here we have one for abbreviated days of the week and that continues on to some of these other ones like American states and various other kinds of person named title acronyms down here but let's go on a little bit further the one that's a bit more interesting than that\r\n okay so he is one of phone numbers this is the kind of ill document in regular expression that's a little bit hard to actually hit your head around for the match used in practice at the very top level of this regular expression things and divided up\r\nby this alternation right here and on the right hand side of the of nation there's a pattern with a separated who is being used as darts and so that once separated out as consistent use of dolts cuz otherwise it's easy for the regular expression to get a role in also recognize various kinds of decimal numbers and all the patterns and so that part of it is actually the easy apart so we can have at the beginning option way that you sir plus signs were too used in Europe and most of the rest of the world has us International prefix country cupboards and then we can have the country code he which is just numbers of the range to the full and then all of that is optional\r\n and then after that\r\ngot a first set of numbers which can be the area code the Doss than the second set of numbers which I guess it starts gives the exchange and then finally the third set of numbers answer these sets of numbers and then being given a link so this has to be between three and four numbers this has to be three and five numbers and the area code has to be too and phone numbers in size range is a chosen said that that work with the phone numbers of a bunch of the countries around the world but if you know what we were international phone numbers don't realize there are actually some cases that went still be recognized I'm by those then what then if we go to the left hand side of the regular expression it's effectively doing the same thing but this more complex\r\nso that the first part of it is a game going to recognize things like optional country code so you can see the same piece over here\r\n I'm too comfy chair so you can actually sort of have some numbers that are put inside averitt parentheses and for the ball and we got this character class where we allowing a variety of other separate is apothem. So we can have a dash switch again needs to be escaped that can just be a space or they can be a nonbreaking space it is a Unicode Escape for non breaking space and so then over all this will allow to recognize a bunch\r\nformats for phone numbers so don't recognize almost all American phone numbers in generator has pretty well with things like UK and Australian phone numbers don't example of where it doesn't work the normal phone number for mass in France did you just have pairs of digits with spaces in between them and us not included here and the difficulty is instead of writing a regular expression that matches that it's in the context of a deterministic regular expression matcha of managing to write one which was wonderful serve only match various other things such as numbers that just appearing as a sequence of numbers to some of the reason\r\nwell I hope that's giving you some idea of the use of regular expressions in NOP systems around another man o p systems I'm sure you'll find lots of other examples\r\ncommonly when people want to match particular patterns when they be patterns of the level of words or patterns of the level of parts of speech they can just be very convenient and practical methods to solve many practical tasks\r\n"]
["output7.txt", "once we've segmented out words or tokenize them we need to normalize them and stem them so normalizing means different things for information retrieval for example we require that the index tax in the query terms have to have the same forms we want to match U. S. A. To USA if somebody asks a question or a query with one of them and the answer has the other we want them to match so it's like implicitly defining some kind of equivalence class of terms\r\nwe might do this by always deleting. For example in my pic they have a rule that takes you. Ask that ETA to USA I'm an alternative is some kind of asymmetric expansion so for example let's say it's we're doing information retrieval if I enter the term window\r\nI might want to search for window or window is there any any any morphological variant of the word window but if I enter capital W Windows I might only want to search for capital W Windows cuz the person is presently looking for the product and not the part of your house\r\n I'm in this is a potentially more powerful algorithm but less efficient than much more complicated so in general we use symmetric and relatively simple expansions so for example in information retrieval we generally remove produce all letters to lowercase since users tend to use lower case with some small exception so for example if we see uppercase in the middle of a sentence like General Motors we might want to keep the case in this matter is for distinguishing and the verb fed from the Federal\r\ntranslate all of the uppercase to lowercase will dealing with a combining all the uppercase and lowercase words and now it's crap out for around finding any line that contains a regular expression in a file very useful Unix program so we're going to look for the regular expression ing dollar sign and them so we'll find all words ending in ing and let's sort them and then we'll just take one copy of each and count them and then sort them by the counts and see what words we find ending in ing in Shakespeare and what we see is lots of these words are not words that in fact we would like to to m\r\n to remove the ing from soap\r\nwords like King and nothing and sing and ring and something and sing and them anything and spring so this is this is this is been a lot of words what a big freaking words in fact that it would be a bad idea to remove the ing remove the 90 from King weed and so on mood I N G from Spring we get spread so what modifier rule\r\n that we did instead of saying grabbing for all words ending in ing let's just go back and change that to grab for all words with a vowel will just make it be aeiou simulator of goggles with Stolen Valor letters\r\n and we need some way to save as a vowel and anything can happen in between followed by the ing what what's how do we say anything.\r\ncharacter * meaning 0 more of those and now it's look at and at what words we get back from that pattern\r\n and now to be specified at the word has to start with a vowel you've done a much better job of finding two syllable words were the ancient package supposed to be stripped off so there's still some problematic words like nothing in something we don't want to get enough and some of them and anything but otherwise I am not coming but otherwise we done a pretty good job of making the rule a little bit better so there's a little explanation of how the porter stemmer works and then how we can actually eat use are eunuchs tools to do a little Corpus Linguistics 2 m to help write rules on this kind\r\n so that's a Sim\r\nexample of morphology it turns out that in some language is much more complex morphology is necessary and Turkish is the famous example of this so here's a word in Turkish which I will be able to pronounce for you I'm which means behaving as if you are among those whom we could not civilize so I assume it's the kind of thing your mother says to you when when you've been particularly naughty and in Turkish this is one word so it's a very long word with a lot of them stems we have the Civilized stem and an ethics mean to become an Ethics meeting cause and I'm not able and so on and so in languages like Turkish and as we saw earlier for the very long nouns in German we're going to have to do a richer and more complex morphine segmentation\r\n Trisomy 18\r\nwhich organization and now we've seen that words will have to be normalized and stems to make them to a normal form\r\nbank with a capital f m or a group like I'm sail the Stanford artificial-intelligence lab from the verb sale and it turns out that for sentiment analysis or machine translation or information extraction cases in fact very helpful it was a big difference between us at the US\r\n we also often want to do lemon ization so we're reducing are inflections or variant forms to their base form so words like a mandar and his will get lemmatized in to be car car car apostrophe s and so on get limit size two car so a phrase like the boys cars are different colors should get Lemon Thai sub boy car be different color so in general the task of limitation is finding the correct dictionary head word formed for\r\nword form that you're given and of course is very important for all sorts of applications for tickling machine translations wear for example if you have a Spanish verb like yet oh I want or kid is you want you it's very important to know that this is the same Lema escudilla the verb to want to this General topic of looking at parts of words leads us to morphology morphology is the study of morphemes in a morpheme is the smallest unit that makes up a word we usually distinguish two kinds of morphemes stems that's the core meaning bearing units time in a word and affixes ethics is the bits and pieces that adhere to the stem off and they have grammatical functions\r\n so on this particular slide in\r\nstem is a stem and ass is an affix the word affix it to confuse us is a stem and their es is the affix so there is no fix this mess and their ass and meaningful is another ethics and so on\r\n so stemming is the task of taking off these offices to reduce terms to their stem and it's particularly historically derive from information-retrieval although it's used in all sorts of applications\r\n we use the word stemming when we specifically mean a kind of a crew chopping off of a bath x's and this is of course a language dependent kind of process so the English word automate automate automatic otamatone we like them all to her\r\nare the same stem automat so stemming is like a simplified version of lemon ization or we pick up a prefix of the word use that to represent the word and we chop off all the suffixes that are relevant leading to that stem\r\n here's an example little text for example compressed and compression are both accepted is equivalent to compress that's the text and if we stemmed that text here's the resulting output so you can see that and that we've lost the Eon example and compressed and compressing have both turned into compressed and here we used our we could have used to be as a representation of our example we used our and so on\r\n the simplest\r\nmost commonly used one for simple standing of English is the porter algorithm\r\n in the port is algorithm is a series of an iterated series of simple rules simple replace rules so I'm free example one set of rules sterile step one na take strings like s s e s and replaces them with SS so in a world like caresses it shops off the es of caresses or rule that takes IES to I chops off the is a ponies and leaves Pony we're going to use Pony with an I come as a representation in Porter stemming of Pony and the rules operate in order so that I'm at this point if there's any SS is left they stay as an s\r\nthe other s's get deleted at this point to the S of cats is deleted while the SS of caress is kept\r\n similarly in Step 1 B we might we remove all of the things in the ad so we want to cross off the ink of walk and the Ed of plastered but we specify the rule very carefully that the porter stemmer only words without a vowel\r\n get their emails remove nuts because of word like sing only words it was a vet in additional Val before the game so weird like seeing which has no extra vowels and only has one Bala Bala ning. Stays a sing but walking which has a vowel and in addition a eval before the aim is allowed to delete the suffix so if a word\r\nhas a vowel followed by eating the English deleted and lots of other sexual Soap Channel turns into 8 so we can cross off the Hall of relational and the Sun and end up with relate and eyes are two eyes and sweet. The are and so on\r\n and the Rose get even more complicated so as you get to very long stems you going to remove it all all of Revival in The Other Boleyn and so on\r\n let's look again at this rule that strips in and practice using the Unix tools that we saw in the last section to look and morphology in a corpus so remember why we stripping the aims only if there's a vowel preceding the end it was the rule\r\nremember we said that I'm in a weird like walking we have a vowel before the aim and so it's okay to remove the Yang in a word like saying there is no vowel before there's no letters at all before they ask there's no previous fouls and so sing the rule doesn't apply\r\n and let's do a little and\r\n search for words ending in ink in Shakespeare so we're going to first take all of Shakespeare and turn all the non alphabetic characters\r\n oops\r\n turn all the knowledge of both characters into new lines are going to get one word per line then we're going to\r\n"]
["output8.txt", "our final discussion in basic text processing is segmenting out sentences from running text\r\n so how we going to send two segments out sentences things that end in! So? That's really great because those are relatively unemployed you excused that we've gotten to the end of a sentence. Unfortunately are quite ambiguous you think about it. Can be a sentence boundary but periods are also used for abbreviations like anchor doctor they're used for numbers like .02 or 4.3 so we can't assume that it. At the end of the sentence so what we need to do to solve the. Problem is build ourselves a classifier we're going to build a binary classify looks at it. And simply makes a binary yes no decision of my at the end of a\r\nam I not at the end of a sentence to make this classifier we could use hand written rules we could use regular Expressions we could build machine learning classifiers the simplest kind of pacifier for this is a decision tree so here's a simple decision tree for deciding whether a word is an end of sentence or not so decision tree is a simple if-then procedure that asks a question and branches based on the answer to the question so we say am I in a piece of text that has a lot of blank lines after me well if so then I'm probably in the end of sentence what if there's no blank lines after me well as my final punctuation a? Or an exclamation point if so well then I'm still probably end up sends well if not that is my final punctuation of.\r\nif it's not well I'm I'm not in a sentence\r\n but if I am a. Well then it depends if I'm on some long list of abbreviations like the word e t c then I'm probably not in the end of sentence I'm just too. Marking an abbreviation like dr. Reedy see but if I'm not an abbreviation then I'm the end of sentence so here's a decision tree\r\n you can imagine arbitrarily sophisticated decision tree features that we could use so one thing we can use is the case or it's called the word shape of the word with a. Am I in uppercase word if my a lowercase word am I all caps uppercase meaning the first letter is uppercase lower meaning it's lowercase cat meaning it's all caps Amaya number any of these kind of weird shape features can give us information and all cow\r\nwhat word is very likely to be a n a f abbreviation we can look at the word with the abbreviate that with the. We can look at the word after the. If the next word starts with a capital letter that I'm likely to be the beginning.. End of sentence because the next word starts with a capital letter and we can look at lots of new features am I a long word or short word so abbreviation tend to be relatively short acronyms tend to be very short them and I can use very sophisticated features so I can say let's look at the the word I'm looking at right now take this word and ask in a corpus that I've already know where the sentence boundaries are how often is this word occur with a.\r\nend of a sentence is the kind of word that ends a sentence is this a kind of word for example that tends to start a sentence is this the word the word that this phrase for example the words after a period very likely to be a capital t h e after. Very likely to start a sentence the space in between them so this will have a high probability of being a start of a sentence and we can use these kind of features depending on condition on each of the words again to help us in deciding what is or isn't end of sentence.\r\nnow a decision tree is just an if-then else statement so that the that that's just the definition of a decision tree is the interesting research is choosing the features so we've seen\r\nnumber of features you might pick for this particular task in general the structure of the Season tree is not as often too hard to build by hand in general hand-building of decision trees is Possible only for very simple features are simple domains you might build a simple decision tree with six or seven rules like this forum for some simple tasks but it's very hard to do you have to pick the threshold for each of the numeric features I'm picking a probability as one of my features I got to have a question in the decision tree is this probability greater than some threshold data or not and I've got I said all those Speedos and so generally we use machine learning that learns the structure of the tree and learns things like the threshold for each of the questions that were asking\r\nnonetheless the questions in a decision tree we can think of them as the kind of features that could be exploited by any other kind of classifier whether it's logistic regression svm's are neural Nets them up as far as we'll talk about it later so this in this intuition that we can build a classifier we can drive feature is that a good predictors of weather app. Is acting as an end up sending this or not and I'm going to put these features into any kind of classifier holds for whatever classify are we going to be using\r\n"]
["output9.txt", "let's begin our discussion of minimum at a distance by defining minimum mid-distance\r\n minimum edit distance is a way of solving the problem of string similarity\r\n how similar are two strings particular examples spell correction the user type g r a f f e what are they really mean and one way of operationalizing this question is asking which of the following words is closer to the letters that they typed graph graft Grail or giraffe tationil biology where we have sequences of nucleotides acgt World we're trying to align and and a good alignment be able to tell us that two particular sequence is perhaps from two samples line up in a search\r\nhey what's um amount of error in this idea of string similarity or sequence similarity comes up for machine translation for information extraction for speech recognition comes up everywhere so let's define the minimum at a distance between two strings is the minimum number of editing operations I'm insertion deletion and substitution that are needed to transform one into the other and we generally use these 3M editing operations insertion deletion the substitution that you can imagine more complicated transpositions and a long-distance movement that we tend to avoid those\r\nso for example we have the string in tension in the string execution here's an alignment showing that many of the letters lineup and with some substitutions and then there was some gaps where\r\nI got your lines up with a letter C and execution and a gap in execution lines up with the letter I and intention and so on\r\n so then we can think about this alignment as having a set of operations that generated the alignment so here the to turn intention and execution we have to delete D4 delete and I we substitute an end for any substitute a t for an X insert a c substituting in for you and the rest of the letters d e the t i o n are all the same so the edit assistance if each operation is one is 5 there's 5 we had to do 5 things to turn intention into execution\r\n if substitutions cost to scald Levinson\r\ndistance and levenshtein distance and insertions and deletions cost one but substitutions cost to and now the distance between these two strings is 8\r\n in competition biology we've seen sequences of bases and our job is to figure out that this is a align to this a mystery to this G and and maybe this t a n t and c a c and so on and so on the seat here so we have some we can see that there's some kind of insertion there we can represent again that alignment between characters by showing this align string of symbols so the task is given to sequences align each letter to a letter when a guy that's on Cascade\r\nedit distance comes up all over the place and machine translation for example we'd like to measure how well a machine translation system does so let's suppose that our machine translation system represented some sentence may be translated from Chinese as the spokesman said the senior advisor was shot dead and some human expert translator said it should have been spokesman confirm senior government advisor was shot so we can measure the difference between these two by saying how many words changed confirms was was substituted with said words that were inserted and the and words and dead and words that were deleted government so it\r\nmeasuring how good are machine translation is by comparing it to humans\r\n similarly in task lightning entity extraction we're going to want to know if IBM ink and IBM are the same entity or Stanford University President John Hennessy is the same entity is Stanford President John Hennessy and we can do that by using at a distance to notice that these are very similar with one word different in here when were different and so by measuring the number of words different we can improve our accuracy Aetna dental extraction and hand and other kinds of tasks are we going to find this minimum distance or intuition for the algorithms to search for a path and it will I pass me a sequence of edits from the start string to the final string so we'll start with an initial state so that would be the word word transforming\r\nwe'll have a set of operators insertion deletion substitution will have a goal State that's the word we're trying to get to\r\n and finally will have a cost for the path of getting there and that's the thing we're trying to minimize that's the number of its that's our path cost so for example from intention he was a piece of a path from intention we could delete a letter in end up with attention we could insert a letter and end up with Ian tension with a substitute a letter and end up with intention so that's these are pieces along a path that would go from intention all the way off the screen here into of all the possible ways you can transform intention into something\r\n so this space of all possible sequences is enormous Diamond so we can't afford to navigate and I believe in this in the sequence\r\nand the intuition of solving this problem of lots of possible sequences is that lots of these pass wined up at the same state so we don't have to keep track of every way of transforming one string into another if the second pieces of a second string or identical all we have to keep is the shortest path to every Revisited States what's an example of how this works we're going to find minimum edit distance formally as for two strings\r\n we have string X of LinkedIn and string y blank m\r\n will Define a distance Matrix capital D of i j\r\n and that would be the edit distance between the first I characters one through I of X and the\r\nfirst J characters 1 through J of string why so that's that that's what's in that supposed to find by D of i j and so the distance between the entire two strings is going to be d a ban come in the springs are blank to NM\r\n what are definition of minimum at a distance\r\n"]
