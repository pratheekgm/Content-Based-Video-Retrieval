hi I'm Dan jurafsky & Chris Manning and I are very happy to welcome you to our course on natural language processing this is a particularly exciting time to be working on natural language processing the vast amount of data on the web and social media have made it possible to build fantastic new applications let's look at one of them question-answering you may know that IBM's Watson won the Jeopardy Challenge on February 16th 2011
 I'm answering questions like William Wilkinson's book inspired this authors most famous novel and you may know that the answer is Bram Stoker who famously wrote
 Dracula
 another important task is information extraction for example of imagine that I have the following email from iCarly Chris about scheduling a meeting
we'd like software to automatically notice that there are dates like tomorrow times like 10 to 11:30 and a room like Gates 159 extract those information create a new calendar entry and then populate a calendar with this kind of structured information with the event date start and end for calendar program in modern email and calendar programs are capable of doing this from text
 another application of this kind of information extraction involve sentiment analysis imagine that you're interested in cameras in your reading a lot of reviews of cameras on the web for here's a bunch of bunch of reviews we'd like to automatically determine from the reviews that would people care about him cameras are particular attributes if they're buying a camera they want to know if
Pinterest so we can see the main purpose of raising don't see but that parse receipt right away is that it's not raises that's the main verb of the sentence but interest somebody interest something and then that's something that gets interested is rates
 what is interesting these rates well
 it's fed raises raises by the Fed so the complete different setting for the different interpretation that something is interesting the rates whatever that could mean and it seems unlikely interpretation for people but of course for a parser this is a perfectly reasonable interpretation that we have to learn how to rule out in fact the sentence can get even more difficult this is the actual headline was somewhat longer so we had
Ed raises interest rates half a percent here we could imagine that rates is the verb and now we have what is rating fed raises interest interest in federal raises R rating half a percent so we might have a dependency structure like this so again interest rates the raises are what do the interesting in the FED is a modifier raises so whether with her face trucks or cars or dependency pars and even more so as we add more words when get more and more ambiguity that have to be solved in order to build a parts for each sentence
the format of the course you're going to have fun video quizzes and most lectures will include a little quiz and they're there just to check basic understanding there simple multiple choice questions you can retake them if you got them wrong
T1 right now number of other things make natural language understanding difficult
 one of them is the non-standard English that we frequently see in text like Twitter feeds or we have capitalization and unusual spelling of words and hashtags and user IDs and so on so all of our partners in part of speech tigers that were going to make you supper often trained on very clean a newspaper text English but the actual English in there in the wild or I will call this a lot of problems I have a lot of segmentation problems for example if we see the the string York news part of New York New Haven how do we know the correct segmentation is New York and New Haven so the New York New Haven railroad and not something like
 York - New
this word here is not a word like in dash long after I saw the segmentation problem correctly you have problems with idioms and with new words that haven't been seen before and will also have problems with entity names like the movie A Bug's Life which has English words in it until it's often difficult to know where the movie names starts and ends and it's comes up very often in biology we have genes and proteins named with English words understanding it's very difficult what tools do we need but we need knowledge about language knowledge about the world and a way to combine these knowledge sources so generally the way we do this is to use probabilistic models that are built from language data so for example if we see the word Maison in French for very likely to translate that has the word house in English the other hand if we see the word
Eisenhower in French very unlikely to translate that as the general avocado and training these probably stick models in general can be very hard but it turns out that we can do an approximate job of probabilistic models with rough text features and will introduce those rough to text features as we go
 so I go on the class is teaching KI theory and methods for statistical natural language processing we'll talk about the viterbi algorithm naive Bayes and Max and classifiers who introduced and gram language modeling statistical parsing we'll talk about the inverted index and tf-idf in Vector models of meaning that are important information retrieval
 will do this for practical robust real world applications will talk about information extraction about spelling correction about information retrieval
the skills you need for the task you'll need simple linear algebra so you should know what a vector is and what a matrix is should have some basic probability Theory need to know how to program an either Java or python because they'll be weekly programming assignments you have your choice of languages
 we're very happy to welcome you to our course on natural language processing and we look forward to seeing you in following lectures
Zoomer affordability your size and weight Buttes and then we'd like to automatically for any particular action to determine how the reviewers felt about those attributes for example if a reviewer said nice and compact to carry that's a positive sentiment and there's another positive example but a phrase like flimsy is a negative sentiment which sentence with the sentiment is and then aggregate for each features for a safe resume for Ford abilities with might decide that this camera reviews really like the flash but they weren't so happy about the use of you so we might measure the positive and negative sentiment about each attribute and then aggregate those machine translation is another important new application and translation can be fully automatics
example we might have a source sentence in Chinese and he was at Stanford's phrasal and t-system translating guide into English but empty can also be used to help human translators so here we might have an Arabic text and the human translator translated into English might need some help from the Mt system for example of a collection of possible next words that the empty system can build automatically and help the human translator let's look at the state of the art in language technology like every field and he's divided up into Specialties and subspecialties number of these problems are pretty close to solve so for example spam detection well it's very hard to completely to text spam you her email boxes we don't have 99% spam and that's because spam detection is a relatively easy classification task
a couple of important component tasks part of speech tagging a named entity tagging will talk about those later in the course and those work at pretty high accuracy using to get 97% accuracy and part of speech tagging and we see how that's important for parts and we're making good progress not as commercial not as completely solved but there are systems out there that are that are being used so we talked about sentiment analysis the task of deciding thumbs-up or thumbs-down on the sentence or product
 component Technologies like Word Sense disambiguation deciding if we're talking about a rodent or a computer mouse when people talk about mouses in a search then we'll talk about parsing which is good enough now to be using lots of applications and machine translation usable on the web applications however are still quite hard
for example answering hard questions like how effective is this medicine in treating. Disease by looking at the Weber by summarizing information we know it was quite hard similarly when we made some progress on the siding that the sentence XYZ company acquired ABC company yesterday mean something similar to ABC has been taken over by XYZ the general problem of detecting that two phrases or sentences mean the same thing to paraphrase to ask still quite hard even harder is the task of summarization reading a number of what's a news article that say oh the Dow Jones up or the S&P 500 is jumped and housing prices Rose and aggravating that to give a user information like in summary the economy is good
 and finally one of the hardest tasks in natural language processing
carrying on a complete human machine communication and dialogue so here's a simple example asking about what movie is playing when and buy movie tickets and you can get applications that do that today the general problem of understanding everything the user might ask for and returning a sensible response is quite difficult
 why is natural language processing so difficult are the kinds of ambiguity problems that are called crash blossoms Lamborghinis any case were surface for might have multiple interpretations a crash Blossom is the name for a kind of headline that has two meanings and ambiguity causes humorous interpretation So reading this first headline violinist link to JL crash blossoms
you might think that the main verb is linked and a violinist is being linked to what he's been linked to Japan Airlines crash blossoms what what are crash blossoms well this headline gave the name to this phenomenon Because the actual interpretation that the headline writer intended the main verb was blossoms who does the blossoming a violinist and this fact about being linked to JL crash with a modifier violinist
 similar kinds of syntactic ambiguity so here teacher strikes Idol kids the writer intended the main verb to be idle the strikes cause the kids to be idle but of course the humorous interpretation is that the teacher is striking strike is the verb and we have a teacher
 striking Idol kids
ambiguity is word since ambiguity
 so when our third example red tape holds up new Bridges the writer intended holds up to mean something like delay call that since one of them holds up but the amusing interpretation is the second sense of holds up which we might write down his to support
 and now we get the interpretation that literal red tape it supposed to be or Craddock red tape is actually supporting a bridge that we can see lots of other kinds of ambiguities in these actual headlines now it turns out that it's not just amusing headlines that have ambiguity ambiguity is pervasive throughout natural language text what's look at a sensible non ambiguous looking headline from The New York Times to the headline with shortened it here it is
and raises interest rates we have a verb here already low parse tree raises what gets raised
 a noun phrase or write a little too nouns here interest rates and we'll have a verb phrase so raising interest rates and then we'll have the Fed
 Megalo noun phrase and I will say this is a sentence that has a noun phrase and a verb phrase raises interest rates so this is called a phrase structure pars we'll talk about that later in the course phrase structure
 so we can also write a dependency far so we see the head verb raises has an argument which is fed and has another dependent which is rates and rates have another itself has a depend
